{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Intro to pytorch","version":"0.3.2","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"TT0Ucfjjam1Z","colab_type":"text"},"source":["#Tensors and gradients"]},{"cell_type":"markdown","metadata":{"id":"PaHiUdaWalKb","colab_type":"text"},"source":["Pytorch is taht kind of dl library which provides us with dynamic graph"]},{"cell_type":"markdown","metadata":{"id":"ZeeoHLmza7oa","colab_type":"text"},"source":["There are several atributes related to gradients that every tensor has\n","\n","  1. **grad**: A property which holds a tensor of the same shape containing computed gradients\n","  2.**is_leaf**: **True**, if this was constructed by a user and **False** if the object is a result of fuction transformation\n","  3.**requires_grad**:**True** if this tensor requires gradients to be calculated/ This property is inherited from leaf tensors which get this value from the tensor construction step (torch.zeros() or torch.tensor() and so on). By default, the constructor has **requires_grad**=**False**, so if you want gradients to be calculated for your tensor, then you need to explicitly say so/"]},{"cell_type":"code","metadata":{"id":"CYO7dqKoclq1","colab_type":"code","colab":{}},"source":["v1 = torch.tensor([1.0, 1.0], requires_grad = True)\n","v2 = torch.tensor([2.0, 2.0])\n","\n","v_sum = v1 + v2\n","v_res = (v_sum*2).sum()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"k_1IslBadViL","colab_type":"text"},"source":["The only leaf tensors here are v1 and v2 and all requires gradients to be calculated except v2.\n","\n","\n","And now let's tell pytorch to calculate the gradients of our graph"]},{"cell_type":"code","metadata":{"id":"pokMPa8Kcys2","colab_type":"code","colab":{}},"source":["v_res.backward()\n","v1.grad"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PvG1mYxieNCB","colab_type":"text"},"source":["By this we ask to calculate derivatives to v_res with respect to any other variables in the graph in our case with respect to v1 and v2, and the result will be a tensor 'that will tell us how much will the v_res increase by adding 1 to our variables"]},{"cell_type":"markdown","metadata":{"id":"L4gTpSy-fk9d","colab_type":"text"},"source":["#NN building blocks"]},{"cell_type":"markdown","metadata":{"id":"S7Fklbu5f0fw","colab_type":"text"},"source":["##Package: torch.nn"]},{"cell_type":"markdown","metadata":{"id":"OfFClqJdgDQI","colab_type":"text"},"source":["The Linear class implements a feed-forward layer with optional bias"]},{"cell_type":"code","metadata":{"id":"GC5wfHmZeobq","colab_type":"code","colab":{}},"source":["import torch.nn as nn\n","l = nn.Linear(2,5)\n","v = torch.FloatTensor([1,2])\n","l(v)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"X2ZjK9vOgjai","colab_type":"text"},"source":["Here we created a linear layer with two inputs and five outputs/ All classes in torch.nn packages inherit from the nn.Module base class? which you can you to implement your own higher-level NN blocks\n","\n","1. **parameters()**: A function that reurns iterator of all variables which require gradient computation (that is, module wights)\n","2. **zeero_grad()**: This function initializes all gradients off all parameters to zero\n","3. **to(device)**: This moves all module parameters to a given device (CPU or GPU)\n","4. **state_dict()**: This returns the dictionary with all module parameters and is useful for modl serialization\n","5. **load_state_dict()**: this initializes the module with the state dictionary"]},{"cell_type":"code","metadata":{"id":"sPxqaW_PhyHf","colab_type":"code","colab":{}},"source":["s = nn.Sequential(\n","nn.Linear(2,5),\n","nn.Relu(),\n","nn.Linear(5,20),\n","nn.Relu(),\n","nn.Linear(20,10),\n","nn.Dropout(p=0.3),\n","nn.Softmax(dim=1))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Kko7VDBAisoK","colab_type":"text"},"source":["Here we defined a three layer NN with softmax on output applied along dimenson 1 (dimenson 0 is batch samples), Relu nonlinearities and dropout. Let's push smth through it:"]},{"cell_type":"code","metadata":{"id":"ZWlWLbqkjB7Q","colab_type":"code","colab":{}},"source":["s(torch.FloatTensor([[1,2]]))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mvrptLtMjLPe","colab_type":"text"},"source":["Dimenson 1 -------->\n","\n","dimenson  0\n","\n","|\n"," \n"," |\n"," \n"," |\n"," \n"," |\n"," \n","v"]},{"cell_type":"markdown","metadata":{"id":"w5JOdeqj26B0","colab_type":"text"},"source":["##Custom layers"]},{"cell_type":"markdown","metadata":{"id":"dDc2a8OF26Iy","colab_type":"text"},"source":["At its core nn.Module provides quite rich functionality to its children\n","1. It tacks all submodules that the current module includes/ For example your building block can have two feed-forward layers used somehow to perform the blocks transformation\n","\n","2. It provides functions to deal with all parameters of the registered submodules. You can obtain a full list of the module's parameters (**parameters()** method), zro it's gradients(**zero_grads()** method) move to Gpu or Cpu, serialize and desereialize the module (state_dict() and load_state_dict() and even perform transformations using your own callable(apply() method)\n","\n","3. It esteblishesthe convention of module application to data. Every module needs to perform its data transformation in the **forward()** method by overriding it\n","\n","4. There are some more functions, such as the ability to register a hook function to tweak module transformation or gradients flow, but it's more for advanced use cases\n","\n","\n","Usually to createa custom module, we have to do only two things: register submodules and implement the **forward()** method.\n","\n","###Example:"]},{"cell_type":"code","metadata":{"id":"c4VFwoGqjW22","colab_type":"code","colab":{}},"source":["class OurModule(nn.Module):\n","  def __init__(self, num_inputs, num_classes, dropout_prob):\n","    super(OurModule, self).__init__()\n","    self.pipe = nn.Sequential(\n","    nn.Linear(num_inputs, 5),\n","    nn.Relu(),\n","    nn.Linear(5,20),\n","    nn.Relu(),\n","    nn.Linear(20, num_classes),\n","    nn.Dropout(p=dropout_prob),\n","    nn.Softmax()\n","    )\n","    \n","  def forward(self, x):\n","    return self.pipe(x)\n","  \n","if __name__ == \"__main__\":\n","  net = OurModule(num_inputs=2, num_classes=3)\n","  v = torch.FloatTensor([[2,3]])\n","  out = net(v)\n","  print(net)\n","  print(out)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PVD0s-989RGw","colab_type":"text"},"source":["##Loss functions"]},{"cell_type":"markdown","metadata":{"id":"2XjBb6mr9Z5s","colab_type":"text"},"source":["###Package: torch.nn"]},{"cell_type":"markdown","metadata":{"id":"CNGNJIkZ9eMT","colab_type":"text"},"source":["The most commnly used are:\n","1. nn.MSELoss: mean square  error between arguments standart loss for regression problems\n","\n","2. **nn.BCELoss and nn.BCELossWIth Logits**: binary cross-entropy loss for binary clasification problems\n","\n","3. **nn.CrossEntropyLoss** and **nn.NLLLoss**: Famous  maximum likelihood criteria which is used in multi classification problems.\n","Tht first version expects raw values and applies LogSoftmax internally, while the second expects to have log probabilities as the input"]},{"cell_type":"markdown","metadata":{"id":"rOaRFJ_7_n04","colab_type":"text"},"source":["##Optimizers\n","\n","###Package: torch.optim\n","\n","\n","1. SGD\n","\n","2. RMSprop\n","\n","3. Adagrad"]},{"cell_type":"markdown","metadata":{"id":"xMbyOP8BBvcB","colab_type":"text"},"source":["#Common lue print of the training loop"]},{"cell_type":"code","metadata":{"id":"7iavZNg19TTA","colab_type":"code","colab":{}},"source":["for batch_samples, batch_labels in iterate_batches(data, batch_size=32):\n","  batch_samples_t = torch.tensor(batch_samples)\n","  batch_lables_t = torch.tensor(batch_lables)\n","  out_t = net(batch_samples_t)\n","  loss_t = loss_function(out_t, batch_labels_t)\n","  loss_t.backward()\n","  optimizer.step()\n","  optimizer.zero_grad()\n","  "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c8CDvLzzM0G6","colab_type":"text"},"source":["##Monitoring with tensorboard\n","\n","\n","What to observe during training of a neural network:\n","\n","1. Loss value, which consists of several components like base loss and regulariztion losses. You shold monitor both total loss and individual components over time\n","\n","2. Results of validation on trining and test sets\n","\n","3. Statistics about gradients and weights\n","\n","4. Learning rates and other hyperparameters, if they are adjusted over time"]},{"cell_type":"code","metadata":{"id":"YT5l9cK3M307","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":190},"outputId":"87a68400-d8d1-4dfd-e3e1-938b8d0613cf","executionInfo":{"status":"ok","timestamp":1564764306940,"user_tz":-180,"elapsed":6143,"user":{"displayName":"Alex Korsakov","photoUrl":"","userId":"18248625518508782982"}}},"source":["!pip install tensorboard-pytorch"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting tensorboard-pytorch\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6e/d6/b8540153f69a8720b2f032fe8c7504ee66c8c0bce9103c272bd67c8e8c77/tensorboard_pytorch-0.7.1-py2.py3-none-any.whl (72kB)\n","\u001b[K     |████████████████████████████████| 81kB 3.2MB/s \n","\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboard-pytorch) (1.12.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboard-pytorch) (1.16.4)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from tensorboard-pytorch) (3.7.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->tensorboard-pytorch) (41.0.1)\n","Installing collected packages: tensorboard-pytorch\n","Successfully installed tensorboard-pytorch-0.7.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"e60gZ_0VOGXA","colab_type":"code","colab":{}},"source":["import math\n","from tensorboardX import SummaryWriter\n","\n","if __name == \"__main__\":\n","  writer = SummaryWriter()\n","  \n","  .\n","  .\n","  .\n","  writer.add_scalar(name, vaql, angle)\n","  \n","  writer.close()\n","  \n","  #takes three parameters: name of the parameter, it's value and current iteration\n","  \n","  \n","  rl_book_samples/Chapter03$ tensorboard --logdir runs --host localhost\n","TensorBoard 0.1.7 at http://localhost:6006 (Press CTRL+C to quit)\n","Now you can open http://localhost:6006 "],"execution_count":0,"outputs":[]}]}